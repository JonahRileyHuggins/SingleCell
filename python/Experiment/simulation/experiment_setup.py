#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
script name: run_benchmark.py
Created on Fri Jan. 10th 12:00:00 2024
Author: Jonah R. Huggins

Description: This script acts as a wrapper for the SPARCED model unit test
                simulation. The script is designed to broadcast the PEtab files
                to all processes, load the SBML model, create a list of unique
                conditions, assign tasks to ranks based on the number of jobs and
                the number of ranks, send the results to the root rank, and save
                the results. If applicable, the script will iterate through the
                observable calculator and save any experimental data with the
                observable-results. If a visualization dataframe is provided, the
                script will generate a unit test plot from the visualization dataframe.

Output: The results of the SPARCED model unit test simulation, saved as a pickle file
        and a unit test plot generated by matplotlib

"""
# -----------------------Package Import & Defined Arguements-------------------#

import os
import yaml
import pickle
from datetime import date
import job_organization as org
from shared_utils.arguments import parse_args
from utils import Utils
from single_simulation import Simulator
from observable_calc import ObservableCalculator
from visualization.visualization import Visualizer
from sedml import builders

args = parse_args()

wd = os.path.dirname(os.path.abspath(__file__))

class RunExperiment:
    """Input the PEtab files and broadcast them to all processes. Then, load
        the SBML model and create a list of unique conditions. Assign tasks
        to ranks based on the number of jobs and the number of ranks. Send
        the results to the root rank and save the results. If applicable,
        iterate through the observable calculator and save any experimental
        data with the observable-results.
    input:
        yaml_file: str - path to the YAML file
        observable: int - 1 for run with observable,
                    0 for run without observable
        name: str - name of the file to save the results
        model_path: str - path to the model directory
        benchmark: str - benchmark to evaluate the model against

    output:
        returns the results of the SPARCED model unit test simulation
    """
    def __init__(self, yaml_path: str):

        try:
            # yaml_path = os.path.join(wd, args.benchmark)
            yaml_path = os.path.abspath(yaml_path)

            assert os.path.exists(yaml_path)

        except AssertionError:
            raise FileNotFoundError(f"{yaml_path} is not a valid benchmark")

        self.yaml_file = yaml_path
        self.benchmark = args.benchmark
        self.observable = int(args.Observable)
        self.name = args.name

        self.communicator, self.rank, self.size = org.mpi_communicator()

        (self.sbml_file,  # SBML file
        self.conditions_df,
        self.measurement_df,
        self.observable_df,
        self.parameters_df,  
        self.visualization_df) = org.broadcast_petab_files(
            self.rank, self.communicator, self.yaml_file
        )
        # Pause to allow for the broadcast to complete
        self.communicator.Barrier()

        # Catalogue each rank's list of tasks at root (rank 0)
        if self.rank == 0:

            # Results dictionary is initialized prior to simulation for convenience
            self.results_dictionary = Utils.results_dictionary(
                self.conditions_df, self.measurement_df
            )

    def run(self):
        """Run the SPARCED model unit test simulation

        Arguements:
            self: object - the RunBenchmark object

        Returns:
            results: dict - the results of the SPARCED model unit test simulation
        """

        # Determine the number of rounds and the directory of tasks for each rank
        rounds_to_complete, rank_jobs_directory = org.task_organization(
            self.rank,
            self.size,
            self.communicator,
            self.conditions_df,
            self.measurement_df,
        )
        # For every cell and condition, run the simulation based on the number of rounds
        for round_i in range(rounds_to_complete):

            if self.rank == 0:
                print(f"Round {round_i+1} of {rounds_to_complete}")

            task = org.task_assignment(
                rank=self.rank,
                size=self.size,
                communicator=self.communicator,
                rank_jobs_directory=rank_jobs_directory,
                round_i=round_i,
                conditions_df=self.conditions_df,
                measurement_df=self.measurement_df,
            )

            if task is None:
                print(f"Rank {self.rank} has no tasks to complete")
                continue

            condition, cell, condition_id = Utils.condition_cell_id(
                task, self.conditions_df, self.measurement_df
            )

            print(f"Rank {self.rank} is running {condition_id} for cell {cell}")

            # Run the simulation for the given condition
            simulator = Simulator(
                yaml_file=self.yaml_file,
                conditions_df=self.conditions_df,
                measurement_df=self.measurement_df,
                parameters_df=self.parameters_df,
                sbml_file=self.sbml_file
            )

            results = simulator.run(condition)
            
            # Results are packaged into a single object to reduce the number of items sent via MPI
            parcel = org.package_results(
                results=results,
                condition_id=condition_id,
                cell=cell,
            )

            if self.rank == 0:

                # Store rank 0's results prior to storing other ranks
                self.results_dictionary = org.store_results(
                    results_dict=self.results_dictionary, individual_parcel=parcel
                )

                # Define the total number of jobs for the results aggregation stage
                total_jobs = Utils.total_tasks(self.conditions_df, self.measurement_df)

                # Collect results from other ranks and store in results dictionary
                self.results_dictionary = org.aggregate_other_rank_results(
                    size=self.size,
                    communicator=self.communicator,
                    results_dict=self.results_dictionary,
                    round_i=round_i,
                    total_jobs=len(total_jobs),
                )
            else:
                # All non-root ranks send results to rank 0
                self.communicator.send(parcel, dest=0, tag=round_i)

            print(f"Rank {self.rank} has completed {condition_id} for cell {cell}")

        return self

    def save_results(self):
        """Save the results of the simulation to a file
        input:
            results: dict - the results of the simulation
            name: str - the name of the file to save the results
        output:
            returns the saved results as a nested dictionary within
            a pickle file
        """

        # Benchmark results are stored within the specified model directory

        results_directory = os.path.join(os.path.dirname(self.yaml_file), "results")

        if not os.path.exists(results_directory):
            os.makedirs(results_directory)

        # Final output is saved in pickle format
        results_path = os.path.join(results_directory, f"{date.today()}.pkl")

        if self.name is not None:
            results_path = os.path.join(results_directory, f"{self.name}.pkl")

        with open(results_path, "wb") as f:
            pickle.dump(self.results_dictionary, f)


    def observable_calculation(self) -> dict:
        """Calculate the observables and compare to the experimental data.
        input:
            results: dict - results of the SPARCED model unit test simulation
        output:
            returns the results of the SPARCED model unit test simulation
        """

        if self.rank == 0 and self.observable == 1:

            self.results_dictionary = ObservableCalculator(self).run()

            RunExperiment.save_results(self)

            return # Proceeds to next command provided in launchers.py

        if self.rank == 0 and self.observable == 0:
            RunExperiment.save_results(self)
            return # Proceeds to next command provided in launchers.py

        if self.rank != 0:
            return None

    def run_visualizer(self):
        """Generate a unit test plot from the visualization dataframe
        input:
            petab_files_data: dict - dictionary of PEtab files

        output:
            returns a unit test plot generated by matplotlib
        """

        if self.rank == 0 and self.visualization_df is not None:

            print("Generating Benchmark Plot")

            results_directory = os.path.join(os.path.dirname(self.yaml_file), "results")

            fig = Visualizer(
                yaml_file=self.yaml_file,
                results_dict=self.results_dictionary,
                visualization_df=self.visualization_df,
                observable_df=self.observable_df,
                measurement_df=self.measurement_df,
            ).dynamic_plot()

            name = self.name if self.name is not None else date.today()
            fig.savefig(os.path.join(results_directory, f"{name}.png"))


    def return_sedml(self) -> None:
        """
        Uses SEDML-mapped functions to generate a SED-ML file from the\
        PEtab files and SBML model. 

        Parameters:
        - args.return_sedml: bool - True if the SED-ML file is to be returned, False otherwise

        Returns:
        - None
        """
        if args.return_sedml:
            builders.build_sedml_file(yaml_file=self.yaml_file)